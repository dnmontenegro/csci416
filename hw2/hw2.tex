
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{amssymb}

\title{CS416 -- HW2}
%\author{liqun.wm }
\date{}
\setlength\parindent{0pt}
\setlength{\parskip}{0.5em}

\begin{document}

\maketitle


\section*{Introduction}
All problems are designed by Dan Sheldon. 

%Due  Friday  10/2  at  11:59pm.  
Please complete all exercises and problems below.

\begin{itemize}

\item All the files can be found in 

\begin{verbatim}
http://www.cs.wm.edu/~liqun/teaching/cs416/hw2/
\end{verbatim}

%\item You can also download the zip file:

%\begin{verbatim}
%http://www.cs.wm.edu/~liqun/teaching/cs416_21f/hw2/hw2.zip
%\end{verbatim}

\item  You can also copy to your directory on a department machine by:

\begin{verbatim}
cp ~liqun/public_html/teaching/cs416/hw2/* .
\end{verbatim}

\end{itemize}


{\noindent} Your  submission  consists  of  three  steps:
\begin{enumerate}

\item  Create hw2.pdf with your solutions to the following problems.  
%The solutions can be typed or written and 
%scanned but the resulting pdf must  be high quality and easily readable.  
Put your name in the file.

\item    You’ll need to create or edit these files in the directory hw2. Complete the requested code in these files.

\begin{itemize}
\item exercise\_2.ipynb

\item logistic\_regression.py

\item one\_vs\_all.py

\end{itemize}

\item Compile your \textbf{exercise\_2.ipynb} to a pdf named \textbf{exercise\_2.pdf}.

\item Submit on Gradescope:

\begin{itemize}

\item \textbf{hw2.pdf}

\item \textbf{exercise\_2.pdf}

\item \textbf{exercise\_2.ipynb}

\item \textbf{logistic\_regression.py}

\item \textbf{one\_vs\_all.py}

\end{itemize}

%\item Submit: 
\begin{verbatim}

\end{verbatim}

\end{enumerate}

\newpage

\section*{Problem 1 (15 points).} {\bf Logistic Regression}


\noindent Let $g(z)=\frac{1}{1+e^{-z}}$ be the logistic function. 

{\bf 1.1 (5 points).} Show that $\frac{d}{dz}g(z)=g(z)(1-g(z))$.

{\bf 1.2 (5 points).} Show that $1-g(z)=g(-z)$. 

{\bf 1.3 (5 points).} Consider the log loss function for logistic regression simplified so there is only one training example: 

$$J(\theta)=-y\log h_\theta (x)- (1-y)\log (1-h_\theta (x))\text{, } h_\theta (x)=g(\theta^T x)=\frac{1}{1+e^{-\theta^T x}}$$

\noindent Show that the partial derivative with respect to $\theta_j$ is:

$$\frac{\partial}{\partial \theta_j} J(\theta)= (h_\theta(x)-y)x_j$$


\section*{Problem 2 (10 points).} {\bf Logistic regression for book classification}. In this problem, you will implement logistic regression for book classification. 

Open \textbf{exercise\_2.ipynb} and follow the instructions to complete the problem.

\section*{Problem 3 (10 points).} {\bf SMS spam classification}. In this problem you will use your implementation of logistic regression to create a spam classifier for SMS messages. 

Open \textbf{exercise\_2.ipynb} and follow the instructions to complete the problem.


%\section*{Problem 6 (5 points extra credit).} Use your own data—either SMS or email—to create a personalized spam classifier. You can either put it in the same format as sms.txt and use code in sms\_classify.ipynb to build the dictionary and features, or you can start with your own format and figure out how to use sklearn.feature\_extraction.text.CountVectorizer to process data in your particular format.

\section*{Problem 4 (34 points).} \textbf{Hand-Written Digit Classification.} In this assignment you will implement one-vs-all multiclass logistic regression to classify images of hand-written
digits. Then you will explore the effects of regularization and training set size on training and test accuracy.

Open \textbf{exercise\_2.ipynb} and follow the instructions to complete the problems.


\end{document}
